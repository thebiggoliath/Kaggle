{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.autograd as autograd\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "\n",
    "\n",
    "import random\n",
    "\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch = 3\n",
    "special_token = 2 #Unknown word, embedding\n",
    "max_len = 100\n",
    "batch_size = 100\n",
    "batch_per_epoch = 1500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace(string):\n",
    "    c = '!#$%^&*(.)[]{};:,/\"<>?-`@\\'~”—=_·\\n+123456“7890‘’'\n",
    "    for i in range(len(c)):\n",
    "        string = string.replace(c[i],\" \")\n",
    "    return(string)\n",
    "data_set = pd.read_csv('../src/train.csv')\n",
    "test_set = pd.read_csv('../src/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = replace(data_set[\"comment_text\"].str.cat(sep = \"|zkr\")).split(\"|zkr\")\n",
    "data_test = replace(test_set[\"comment_text\"].str.cat(sep = \"|zkr\")).split(\"|zkr\")\n",
    "test_id = test_set[\"id\"].str.cat(sep = \"|\").split(\"|\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = list(\" \".join(d.split()) for d in data)\n",
    "test_sentence = list(\" \".join(d.split()) for d in data_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = set((\"\".join([j for i in sentence for j in i])).split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_to_ix = defaultdict(int)\n",
    "for i,word in enumerate(words):\n",
    "    word_to_ix[word] = i+special_token\n",
    "word_to_ix[\"<unk>\"] = 0\n",
    "word_to_ix[\"<pad>\"] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_to_dict(sentence,max_len,word_to_ix):\n",
    "    s_split = sentence.split()\n",
    "    indi = [word_to_ix[word] for word in s_split]\n",
    "    if len(indi) < max_len:\n",
    "        indi += [word_to_ix[\"<pad>\"]] * (max_len - len(indi))\n",
    "    else:\n",
    "        indi = indi[:max_len]\n",
    "    return indi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_batch(sentences,max_len,word_to_ix,batch_size,data_set):\n",
    "    correct = []\n",
    "    sentence = []\n",
    "    for _ in range(batch_size):\n",
    "        index = random.randint(0,len(sentences)-1)\n",
    "        sentence.append(sentences[index])\n",
    "        correct.append([data_set[\"toxic\"][index],data_set[\"severe_toxic\"][index],data_set[\"obscene\"][index],data_set[\"threat\"][index],data_set[\"insult\"][index],\\\n",
    "                        data_set[\"identity_hate\"][index]])\n",
    "    sen_idx = [sentence_to_dict(sen,max_len,word_to_ix) for sen in sentence]\n",
    "    return sen_idx,correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_batch(test_sentence,max_len,word_to_ix,batch_size,test_id,test_index):\n",
    "    test_id_batch = []\n",
    "    test_sentence_batch = []\n",
    "    end = test_index+batch_size if test_index+batch_size < len (test_id) else len (test_id)\n",
    "    for i in range(test_index,end):\n",
    "        test_id_batch.append(test_id[i])\n",
    "        test_sentence_batch.append(test_sentence[i])\n",
    "    test_sen_idx = [sentence_to_dict(sen,max_len,word_to_ix) for sen in test_sentence_batch]\n",
    "    return test_sen_idx,test_id_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self,vocab,max_len,special_token,output_nb = 12,dim_emb=40,batch_size):\n",
    "        super(CNN, self).__init__()\n",
    "        self.max_len = max_len\n",
    "        self.dim_emb = dim_emb\n",
    "        self.embedding = nn.Embedding(len(vocab)+special_token, dim_emb)\n",
    "        self.conv1 = nn.Conv1d(100, 50, 2)\n",
    "        self.linear = nn.Linear(max_len*dim_emb,output_nb)\n",
    "    def forward(self, vector):\n",
    "        p = self.embedding(vector)\n",
    "        p = F.relu(self.conv1(p))\n",
    "        p = p.view(-1,self.max_len*self.dim_emb)\n",
    "        p = self.linear(p)\n",
    "        return p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = CNN(words,max_len,special_token,batch_size)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()#\n",
    "\n",
    "optimizer = optim.Adagrad(clf.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "invalid argument 2: size '[-1 x 7000]' is invalid for input with 345000 elements at ..\\src\\TH\\THStorage.c:37",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-57-73f7125e4f29>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mclf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m         \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mview\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mY\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mview\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    489\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    490\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 491\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    492\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    493\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-55-d9af89e8e37b>\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, vector)\u001b[0m\n\u001b[0;32m     10\u001b[0m         \u001b[0mp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0membedding\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvector\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m         \u001b[0mp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconv1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m         \u001b[0mp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mview\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax_len\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdim_emb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m         \u001b[0mp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mp\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: invalid argument 2: size '[-1 x 7000]' is invalid for input with 345000 elements at ..\\src\\TH\\THStorage.c:37"
     ]
    }
   ],
   "source": [
    "avg_loss = 0\n",
    "for e in range(epoch):\n",
    "    for batch_idx in range(batch_per_epoch):\n",
    "\n",
    "        batch,Y = create_batch(sentence,max_len,word_to_ix,batch_size,data_set)\n",
    "\n",
    "        batch_tensor = torch.LongTensor(batch)\n",
    "        Y = torch.LongTensor(Y)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        result = clf(batch_tensor)\n",
    "\n",
    "        loss = criterion(result.view(-1,2),Y.view(-1)) \n",
    "\n",
    "        avg_loss += loss.item()\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch_idx%500 == 0:\n",
    "            print(\"Epoch: \",e,\"|Batch:\",batch_idx,\"|Loss :\", round(avg_loss/500, 8))\n",
    "            avg_loss = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
