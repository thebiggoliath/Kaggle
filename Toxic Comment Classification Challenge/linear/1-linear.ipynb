{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.autograd as autograd\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "\n",
    "\n",
    "import random\n",
    "\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch = 3\n",
    "special_token = 2 #Unknown word, embedding\n",
    "max_len = 100\n",
    "batch_size = 100\n",
    "batch_per_epoch = 1500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace(string):\n",
    "    c = '!#$%^&*(.)[]{};:,/\"<>?-`@\\'~”—=_·\\n+123456“7890‘’'\n",
    "    for i in range(len(c)):\n",
    "        string = string.replace(c[i],\" \")\n",
    "    return(string)\n",
    "data_set = pd.read_csv('./src/train.csv')\n",
    "test_set = pd.read_csv('./src/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = replace(data_set[\"comment_text\"].str.cat(sep = \"|zkr\")).split(\"|zkr\")\n",
    "data_test = replace(test_set[\"comment_text\"].str.cat(sep = \"|zkr\")).split(\"|zkr\")\n",
    "test_id = test_set[\"id\"].str.cat(sep = \"|\").split(\"|\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = list(\" \".join(d.split()) for d in data)\n",
    "test_sentence = list(\" \".join(d.split()) for d in data_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = set((\"\".join([j for i in sentence for j in i])).split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_to_ix = defaultdict(int)\n",
    "for i,word in enumerate(words):\n",
    "    word_to_ix[word] = i+special_token\n",
    "word_to_ix[\"<unk>\"] = 0\n",
    "word_to_ix[\"<pad>\"] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_to_dict(sentence,max_len,word_to_ix):\n",
    "    s_split = sentence.split()\n",
    "    indi = [word_to_ix[word] for word in s_split]\n",
    "    if len(indi) < max_len:\n",
    "        indi += [word_to_ix[\"<pad>\"]] * (max_len - len(indi))\n",
    "    else:\n",
    "        indi = indi[:max_len]\n",
    "    return indi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_batch(sentences,max_len,word_to_ix,batch_size,data_set):\n",
    "    correct = []\n",
    "    sentence = []\n",
    "    for _ in range(batch_size):\n",
    "        index = random.randint(0,len(sentences)-1)\n",
    "        sentence.append(sentences[index])\n",
    "        correct.append([data_set[\"toxic\"][index],data_set[\"severe_toxic\"][index],data_set[\"obscene\"][index],data_set[\"threat\"][index],data_set[\"insult\"][index],\\\n",
    "                        data_set[\"identity_hate\"][index]])\n",
    "    sen_idx = [sentence_to_dict(sen,max_len,word_to_ix) for sen in sentence]\n",
    "    return sen_idx,correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_batch(test_sentence,max_len,word_to_ix,batch_size,test_id,test_index):\n",
    "    test_id_batch = []\n",
    "    test_sentence_batch = []\n",
    "    end = test_index+batch_size if test_index+batch_size < len (test_id) else len (test_id)\n",
    "    for i in range(test_index,end):\n",
    "        test_id_batch.append(test_id[i])\n",
    "        test_sentence_batch.append(test_sentence[i])\n",
    "    test_sen_idx = [sentence_to_dict(sen,max_len,word_to_ix) for sen in test_sentence_batch]\n",
    "    return test_sen_idx,test_id_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Classifier(nn.Module):\n",
    "    def __init__(self,vocab,max_len,special_token,output_nb = 12,dim_emb=70):\n",
    "        super(Classifier, self).__init__()\n",
    "        self.max_len = max_len\n",
    "        self.dim_emb = dim_emb\n",
    "        self.embedding = nn.Embedding(len(vocab)+special_token, dim_emb)\n",
    "        self.linear = nn.Linear(max_len*dim_emb,output_nb)\n",
    "    def forward(self, vector):\n",
    "        p = self.embedding(vector)\n",
    "        p = p.view(-1,self.max_len*self.dim_emb)\n",
    "        p = self.linear(p)\n",
    "        #nl = nn.Softmax(0)\n",
    "        #p = nl(p)\n",
    "        return p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = Classifier(words,max_len,special_token)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()#\n",
    "\n",
    "optimizer = optim.Adagrad(clf.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  0 |Batch: 0 |Loss : 0.00029613\n",
      "Epoch:  0 |Batch: 500 |Loss : 0.08859602\n",
      "Epoch:  0 |Batch: 1000 |Loss : 0.08679001\n",
      "Epoch:  1 |Batch: 0 |Loss : 0.08172032\n",
      "Epoch:  1 |Batch: 500 |Loss : 0.07964151\n",
      "Epoch:  1 |Batch: 1000 |Loss : 0.07610735\n",
      "Epoch:  2 |Batch: 0 |Loss : 0.07317241\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-64-73f7125e4f29>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     16\u001b[0m         \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 18\u001b[1;33m         \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     19\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mbatch_idx\u001b[0m\u001b[1;33m%\u001b[0m\u001b[1;36m500\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\optim\\adagrad.py\u001b[0m in \u001b[0;36mstep\u001b[1;34m(self, closure)\u001b[0m\n\u001b[0;32m     92\u001b[0m                     \u001b[0mstate\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'sum'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maddcmul_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     93\u001b[0m                     \u001b[0mstd\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstate\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'sum'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1e-10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 94\u001b[1;33m                     \u001b[0mp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maddcdiv_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mclr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstd\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     95\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     96\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "avg_loss = 0\n",
    "for e in range(epoch):\n",
    "    for batch_idx in range(batch_per_epoch):\n",
    "\n",
    "        batch,Y = create_batch(sentence,max_len,word_to_ix,batch_size,data_set)\n",
    "\n",
    "        batch_tensor = torch.LongTensor(batch)\n",
    "        Y = torch.LongTensor(Y)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        result = clf(batch_tensor)\n",
    "\n",
    "        loss = criterion(result.view(-1,2),Y.view(-1)) \n",
    "\n",
    "        avg_loss += loss.item()\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch_idx%500 == 0:\n",
    "            print(\"Epoch: \",e,\"|Batch:\",batch_idx,\"|Loss :\", round(avg_loss/500, 8))\n",
    "            avg_loss = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this is gonna change\n",
    "#first no thinking try\n",
    "\n",
    "nl = nn.Softmax(-1)\n",
    "sub = [[] for _ in range(len(test_sentence))]\n",
    "for test_index in range(0,len(test_sentence),batch_size):\n",
    "\n",
    "        batch,identity = validate_batch(test_sentence,max_len,word_to_ix,batch_size,test_id,test_index)\n",
    "\n",
    "        batch_tensor = torch.LongTensor(batch)\n",
    "\n",
    "        result = clf(batch_tensor)\n",
    "        \n",
    "        p = nl(nl(result.view(-1,2)))\n",
    "        \n",
    "        end = batch_size if test_index+batch_size < len (test_id) else len (identity)\n",
    "        for i in range(0,end):\n",
    "            sub[test_index+i].append(identity[i])\n",
    "            for j in range(0,6):\n",
    "                sub[test_index+i].append(p[i*6+j][1].item())\n",
    "\n",
    "submission = pd.DataFrame([i[0] for i in sub], columns=['id'])\n",
    "submission[\"toxic\"] = [i[1] for i in sub]\n",
    "submission[\"severe_toxic\"] = [i[2] for i in sub]\n",
    "submission[\"obscene\"] = [i[3] for i in sub]\n",
    "submission[\"threat\"] = [i[4] for i in sub]\n",
    "submission[\"insult\"] = [i[5] for i in sub]\n",
    "submission[\"identity_hate\"] = [i[6] for i in sub]\n",
    "submission.to_csv('../src/submission.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
